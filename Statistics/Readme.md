# Statistics

Files for the calculation of the CACAPO dataset statistics (and that of E2E and WebNLG for comparison)

<h2>Installation</h2>

To run all statistics scripts, you need the following libraries:

[sacreBLEU](https://github.com/mjpost/sacrebleu/)<br/>
[Regex](https://pypi.org/project/regex/)<br/>
[Sacremoses](https://github.com/alvations/sacremoses/)<br/>
[NLTK](https://www.nltk.org/)<br/>
[BERTScore](https://pypi.org/project/bert-score/)<br/>
[jsonlines](https://pypi.org/project/jsonlines/)<br/>
[spaCy](https://spacy.io/usage/)<br/>
[pySBD](https://github.com/nipunsadvilkar/pySBD)<br/>
[LexicalRichness](https://pypi.org/project/jsonlines/)<br/>
[SoMaJo](https://github.com/tsproisl/SoMaJo)<br/>
[ROUGE metric](https://github.com/li-plus/rouge-metric)</br>

<h2>The scripts</h2>

The following scripts are available:

- Statistics_Corpus. Calculates size metrics for the dataset, more specifically: (1) Number of instances, (2) Number of unique MRs, (3) Instances per MR, (4) Slots per MR, (5) Words per instance, (6) Words per sentence, (7) Sentences per instance.
- Tokens_Types. Calculates lexical richness metrics for the dataset, more specifically: (1) Number of tokens, (2) Number of types, (3) Type-token ratio (TTR), (4) Mean segmental TTR (MSTTR, 25 tokens), (5) Lexical Sophistication (LS).
- Evaluation script. Calculates text quality metrics for the TGen output, as described in the paper, more specifically: (1) BLEU, (2) METEOR, (3) NIST, (4) BertScore.

<h2>Other metrics</h2>

- D-Level scores for English was calculated using  [D-Level Analyzer](http://www.personal.psu.edu/xxl13/downloads/d-level.html) on output generated by Collins' parser. For Dutch, [T-scan](https://github.com/proycon/tscan/) was used to calculate D-Level scores.
- CIDEr was calculated using the [original evaluation script](https://github.com/vrama91/cider/)
